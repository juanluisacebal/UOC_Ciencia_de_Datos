{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "CE2WOrcTNUSd",
        "LxG9is4Vi4Z9"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# PEC 2. Introducción a los sistemas de recuperación de la información.\n",
        "\n",
        "En esta PEC vamos a desarrollar un sistema de recuperación de la información básico. Partiendo de una lista de documentos de texto, deberás utilizar las técnicas de recuperación de la información vistas en la asignatura para obtener, procesar y analizar datos útiles a partir del contenido.\n",
        "\n",
        "\n",
        "## Ejercicio 3: tareas básicas\n",
        "\n",
        "Además de las ya clásicas `pandas` y `numpy`, utilizaremos la librería [NLTK](https://es.wikipedia.org/wiki/NLTK) (Natural Language Toolkit), una librería de Python utilizada para analizar texto y aprendizaje automático."
      ],
      "metadata": {
        "id": "CE2WOrcTNUSd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sk1kBQ3ENpl-",
        "outputId": "6b952fc7-116b-4145-da9a-dace72b0146b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.1 Separando las palabras (Tokenización)\n",
        "\n",
        "El significado de cada sentencia se obtiene de las palabras que contiene. Así que analizando las palabras presentes en un texto puede interpretarse el significado. Así que lo primero que hay que hacer para tratar el texto es separar las palabras que lo componen, es decir, hacer una lista de palabras. El modelo que utilizaremos aquí se denomina [**bolsa-de-palabras**](https://es.wikipedia.org/wiki/Modelo_bolsa_de_palabras) (bag-of-words) ya que nos interesan las palabras sin importar su posición o importancia en el documento.\n",
        "\n",
        "La separación de las palabras o tokenización consiste en separar el texto en palabras, también llamadas tokens. Generalmente, el \"espacio\" se utiliza para separar palabras y elementos como los puntos, comas, dos puntos, etc., se utilizan para separar frases.\n",
        "\n",
        "Existen múltiples formas de realizar la separación de palabras para un texto determinado.\n",
        "\n",
        "### 3.1.1 Funciones de Python\n",
        "\n",
        "Se puede utilizar la función `split()` para separar una cadena de texto en una lista de palabras. De forma predeterminada, `split()` utiliza el espacio en blanco, aunque se puede utilizar cualquier carácter."
      ],
      "metadata": {
        "id": "i2QypGqmNti5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text01 = \"This sentence is the initial example text that illustrates the concept we're discussing.\"\n",
        "text01.split(' ')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NJWmnpyEOQoz",
        "outputId": "651b4259-b40a-40ab-887b-74081eda392c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['This',\n",
              " 'sentence',\n",
              " 'is',\n",
              " 'the',\n",
              " 'initial',\n",
              " 'example',\n",
              " 'text',\n",
              " 'that',\n",
              " 'illustrates',\n",
              " 'the',\n",
              " 'concept',\n",
              " \"we're\",\n",
              " 'discussing.']"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "El método `split()` de Python no considera los signos de puntuación como elementos separados.\n",
        "\n",
        "### 3.1.2 Expresiones regulares\n",
        "\n",
        "El módulo `re` ofrece un conjunto de funciones para buscar coincidencias en una cadena de texto. Una *expresión regular* es una secuencia de caracteres que definen un patrón de búsqueda."
      ],
      "metadata": {
        "id": "1zEukbOvObzD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "text02 = \"\"\"There are multiple ways we can perform tokenization on given text data. We can choose any method based on language, library and purpose of modeling.\"\"\"\n",
        "\n",
        "# TODO: Utiliza una expresión regular para separar las palabras utilizando la librería re\n"
      ],
      "metadata": {
        "id": "4p35jcqNOjIS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.1.3 Con NLTK\n",
        "\n",
        "El Natural Language Toolkit (NLTK) tiene la función `word_tokenize()` para la separación de palabras y `sent_tokenize()` para la separación de frases."
      ],
      "metadata": {
        "id": "lvz6jYyNPilD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "\n",
        "text03 = \"<p>This is the first sentence. A gallon-of-milk in the U.S. costs $2.99. Is this the third sentence? Yes, it is!</p>\"\n",
        "\n",
        "# TODO: separar las frases\n"
      ],
      "metadata": {
        "id": "jv5XFZvFXn-z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.2. Eliminación de números y símbolos. Conversión a minúsculas\n",
        "\n",
        "Como puede comprobarse, `word_tokenizer()` mantiene los signos de puntuación, así como los números y otros símbolos.\n",
        "\n",
        "Una estrategia para reducir el número de palabras/tokens es convertirlas a minúsculas, puesto que algunos signos de puntuación pueden modificar la letra inicial de las palabras. De esta forma, se reduce el número de variantes de una misma palabra."
      ],
      "metadata": {
        "id": "s0t3Vn3iXz7L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "\n",
        "def remove_tags (s):\n",
        "  # TODO: eliminar tags html\n",
        "  return p.sub('-', s)\n",
        "\n",
        "# TODO : Eliminar etiquetas, tokenizar, convertir a minúsculas y eliminar símbolos no alfabéticos y números:\n",
        "def tokenize_and_remove_punctuations(s):\n",
        "    return nltk.word_tokenize( ss.lower() )\n",
        "\n",
        "print ((tokenize_and_remove_punctuations (text03)))"
      ],
      "metadata": {
        "id": "sPuWUEZqY8Sy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.3. Palabras vacías\n",
        "\n",
        "Las *palabras vacías* (stopwords) son las palabras más comunes en cualquier idioma, tienen sentido gramatical pero tienen un significado limitado en el análisis de un texto. Estas palabras vacías incluyen artículos, preposiciones, conjunciones, pronombres, etc., y su eliminación reduce considerablemente el número de palabras.\n",
        "\n",
        "NLTK tiene listas de palabras vacías en 16 idiomas. En este caso, se debe cargar la lista en inglés."
      ],
      "metadata": {
        "id": "q6SsX7fWZBAx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO : dada una lista de tokens, elimina aquellos que sean palabras vacías o que tengan una longitud menor o igual a 2.\n",
        "def remove_stop_words(tokens):\n",
        "    return filtered_words\n"
      ],
      "metadata": {
        "id": "lrdDwhjcX7qt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.4. Normalización\n",
        "\n",
        "Muchos idiomas contienen palabras derivadas de otros, y esto se conoce como [flexión](https://es.wikipedia.org/wiki/Flexi%C3%B3n_(ling%C3%BC%C3%ADstica)). La flexión es la modificación de una palabra para expresar diferentes categorías gramaticales como persona, número, género, etc.\n",
        "\n",
        "El tratamiento de esta flexión para llevar las palabras a una forma base se conoce como normalización de palabras. La normalización permite que, al buscar una palabra, la búsqueda se realice simultáneamente a través de todas sus flexiones.\n",
        "\n",
        "La **lematización** es el proceso de reducir la inflexión de las palabras para llevarlas a su forma original o raíz. El lema es la parte de la palabra a la que se añade la flexión.\n",
        "\n",
        "En NLTK existen diferentes lematizadores disponibles, aunque aquí utilizaremos el más conocido: el [algoritmo de Porter](https://es.wikipedia.org/wiki/Algoritmo_de_Porter).\n",
        "\n",
        "Puede encontrar más información sobre estos procesos en https://www.datacamp.com/community/tutorials/stemming-lemmatization-python."
      ],
      "metadata": {
        "id": "dr9eF7lzYZ5u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "# TODO : obtener la versión normalizada de todos los tokens\n",
        "def stem_words(tokens):\n",
        "    return stemmed_words\n",
        "\n",
        "stem_words ( remove_stop_words(tokenize_and_remove_punctuations (text03)) )"
      ],
      "metadata": {
        "id": "YHndixioYivn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Una vez realizadas las operaciones básicas sobre el texto, es momento de reunirlo todo en la función `preprocess_data()` que recibe un array de pares `(documentId, text)` y aplica las transformaciones anteriormente descritas."
      ],
      "metadata": {
        "id": "T5KrogtUZX2q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#TODO\n",
        "def preprocess_text ( text , stem=True):\n",
        "    return stemmed_tokens\n",
        "\n",
        "def preprocess_data(contents, stem=True):\n",
        "    return dataDict\n"
      ],
      "metadata": {
        "id": "OSWd2Y6SbIzz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "document_1 = \"I love watching movies when it's cold outside ;-)\"\n",
        "document_2 = \"Toy Story is the best animation movie ever, I love it!\"\n",
        "document_3 = \"Watching horror movies alone at night is really scary\"\n",
        "document_4 = \"He loves to watch films filled with suspense and unexpected plot twists\"\n",
        "document_5 = \"My mom loves to watch movies. My dad hates movie theaters. My brothers like any kind of movie. And I haven't watched a single movie since I got into college\"\n",
        "documents = [document_1, document_2, document_3, document_4, document_5]\n",
        "\n",
        "docIds = ['doc01','doc02','doc03','doc04','doc05']\n",
        "\n",
        "# TODO : generar una lista de la forma [('doc01', 'I love..'), ... ]\n",
        "\n",
        "# TODO : preprocesar la lista de documentos de ejemplo\n"
      ],
      "metadata": {
        "id": "I-5SSC26bKz_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.5. Frecuencia de las palabras\n",
        "\n",
        "Ahora veremos la importancia de una palabra/token en los documentos.\n",
        "\n",
        "Lo primero es obtener un vocabulario, que no es más que la lista de todos los tokens únicos que aparecen en todos los documentos."
      ],
      "metadata": {
        "id": "CAI0lezFbiUD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#TODO:\n",
        "def get_vocabulary(data):\n",
        "    return list(fdist.keys())\n",
        "\n",
        "get_vocabulary ( data_docs)"
      ],
      "metadata": {
        "id": "Wo68HUrzbu-7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para ello, podemos calcular la frecuencia de cada término contando el número de veces que aparece en cada documento, que será una medida de su peso o importancia.\n",
        "\n",
        "$TF (t, d) = f_{t, d}$ (número de repeticiones del término $t$ en el documento $d$)"
      ],
      "metadata": {
        "id": "CEwIlnanb4zL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.probability import FreqDist\n",
        "\n",
        "# TODO: calcular tf\n",
        "def calculate_tf(tokens):\n",
        "    tf_score = {}\n",
        "    return tf_score\n",
        "\n",
        "\n",
        "fdist = calculate_tf (data_docs['doc05'])\n",
        "fdist"
      ],
      "metadata": {
        "id": "nn5GCcoJcB5-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "La **frecuencia inversa de documentos** para un término $t$ es el logaritmo (en este caso en base 2) del cociente entre el número de documentos y el número de documentos en los que aparece el término $t$.\n",
        "\n",
        "$ IDF (t) = log_{2} \\frac{N}{\\{d \\in D : t \\in d \\}} $\n",
        "\n",
        "Una puntuación más alta de TF*IDF indica que el término es más específico, mientras que una puntuación menor indica que es más genérico."
      ],
      "metadata": {
        "id": "VZgIcdp6cLuF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "# TODO: Calcula el idf\n",
        "def calculate_idf(data):\n",
        "    idf_score = {}\n",
        "    # TODO: número de documentos\n",
        "    # TODO: obtener el vocabulario\n",
        "    for word in all_words:\n",
        "        word_count = 0\n",
        "        for token_list in data.values():\n",
        "            # TODO\n",
        "            # TODO: calcular idf\n",
        "    return idf_score\n",
        "\n",
        "\n",
        "idf_score = calculate_idf ( data_docs )\n",
        "idf_score"
      ],
      "metadata": {
        "id": "lpkaNmHccYOv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#TODO: haz una función que, dado una lista de documentos, devuelva el tf_idf\n",
        "def calculate_tfidf(data, idf_score):\n",
        "    scores = {}\n",
        "    for key,value in data.items():\n",
        "        # TODO: calcular tf\n",
        "    for doc,tf_scores in scores.items():       # TODO\n",
        "        # TODO\n",
        "    return scores\n",
        "\n",
        "\n",
        "tfidf_score = calculate_tfidf ( data_docs, idf_score)\n",
        "tfidf_score"
      ],
      "metadata": {
        "id": "yXOFZfWEcnZX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.6. Generación del espacio vectorial\n",
        "\n",
        "Utilizando las funciones anteriores, construiremos la matriz de documentos (como filas) y términos (como columnas). Para facilitar esta tarea, utilizaremos una estructura de datos ya conocida, el **dataframe**."
      ],
      "metadata": {
        "id": "vidhyejKhmcN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# TODO: obtener el vocabulario, calcular el 'if_idf' y crear un df con las frecuencias\n",
        "def generate_dataframe ( data ):\n",
        "  # TODO\n",
        "  tf_idf_score = calculate_tfidf ( data , idf_score)\n",
        "  table = []\n",
        "  for doc in tf_idf_score.keys():\n",
        "      # TODO\n",
        "  return df\n",
        "\n",
        "\n",
        "df_data = generate_dataframe (data_docs)\n",
        "df_data.head(5)\n"
      ],
      "metadata": {
        "id": "R_1RTvn2hpqz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.7. Generar el índice invertido\n",
        "\n",
        "De forma similar, generaremos un índice invertido que almacenaremos en una estructura de datos Python: el diccionario."
      ],
      "metadata": {
        "id": "GymfxNxAh9eZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#TODO: llena la función para generar un índice invertido\n",
        "\n",
        "def generate_inverted_index(data):\n",
        "    #TODO\n",
        "\n",
        "    index = {}\n",
        "    for word in all_words:\n",
        "        for doc, tokens in data.items():\n",
        "            #TODO\n",
        "    return index\n",
        "\n",
        "inverted_index = generate_inverted_index (data_docs)\n",
        "\n",
        "inverted_index"
      ],
      "metadata": {
        "id": "x-BOalcMh_RZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.8. Resolución de consultas\n",
        "\n",
        "Resolveremos consultas (obtendremos los documentos más relevantes) considerando la consulta como un vector y comparándolo con el conjunto de documentos mediante la **similitud del coseno**.\n",
        "\n",
        "Para ello utilizaremos la librería [sklearn](https://scikit-learn.org/stable/), aunque sólo utilizaremos la funcionalidad para calcular la similitud del coseno."
      ],
      "metadata": {
        "id": "7jqC28-FiZJw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: generar un dataframe para a la consulta\n",
        "\n",
        "q = \"I watched alone a horror movie\"\n",
        "\n",
        "def generate_query_dataframe ( vocabulary , q ):\n",
        "  # TODO : preprocesar las palabras de la consulta\n",
        "\n",
        "  # TODO : generar un dataframe con la misma estructura\n",
        "  table = []\n",
        "  # TODO : si el token está en la consulta se pone un 1, de lo contrario un 0\n",
        "  return df\n",
        "\n",
        "df_query =  generate_query_dataframe ( get_vocabulary(data_docs)  , q )\n",
        "df_query.head()\n"
      ],
      "metadata": {
        "id": "hktKK1oOieJ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "\n",
        "#TODO : Calcula la similitud del coseno e indicar qué documento es el más cercano a la consulta\n",
        "\n"
      ],
      "metadata": {
        "id": "BApBEYsUiver"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ejercicio 4\n",
        "\n",
        "ClinicalTrials.gov es un registro de base de datos online mantenido por los Institutos Nacional de Salud de Estados Unidos (NIH). Contiene información sobre estudios clínicos de todo el mundo, incluyendo detalles sobre los objetivos, participantes, métodos, resultados y otras informaciones relevantes sobre los estudios.\n",
        "\n",
        "A través de su API abierta, obtendremos información sobre ensayos clínicos reales.  https://classic.clinicaltrials.gov/api/gui/ref/api_urls#urlParams\n",
        "\n",
        "\n",
        "Una API (interfaz de programación de aplicaciones) es un conjunto de herramientas y protocolos que permiten a diferentes programas informáticos comunicarse entre sí y compartir datos o funcionalidades de forma estandarizada y controlada."
      ],
      "metadata": {
        "id": "LxG9is4Vi4Z9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "\n",
        "# Define la URL base para la API de ClinicalTrials.gov\n",
        "base_url = 'https://clinicaltrials.gov/api/query/study_fields?'\n",
        "\n",
        "# Define tus parámetros de búsqueda\n",
        "search_term = 'diabetes' # Cambia esto por tu término de búsqueda deseado\n",
        "max_studies = 15 # Número máximo de estudios a recuperar\n",
        "\n",
        "# Define los campos que deseas recuperar\n",
        "fields = [\n",
        "     'NCTId', # Identificador de ClinicalTrials.gov\n",
        "     'BriefTitle', # Título breve del estudio\n",
        "     'Condition', # Condición o enfermedad objeto de estudio\n",
        "     'InterventionName', # Nombre de la intervención (si se aplica)\n",
        "     'Phase', # Fase del estudio\n",
        "     'OverallStatus' # Estado general del estudio\n",
        "]"
      ],
      "metadata": {
        "id": "EpVOYRrOfZIN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO:Define los parámetros para la solicitud en la API\n",
        "params = {\n",
        "}"
      ],
      "metadata": {
        "id": "NbdyDOzuxGRu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO:\n",
        "# Haz la solicitud en la API\n",
        "response = requests.get(base_url, params=params)"
      ],
      "metadata": {
        "id": "BqUzD4LxxH5f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO:\n",
        "if response.status_code == 200:\n",
        "\n",
        "     # TODO: Analiza la respuesta JSON\n",
        "\n",
        "     # TODO: Construye un DataFrame con los resultados\n",
        "     df = pd.DataFrame(studies)\n",
        "\n",
        "else:\n",
        "     print(\"No se ha podido obtener ningún estudio clínico.\")"
      ],
      "metadata": {
        "id": "l96HzufQsH_g",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        },
        "outputId": "737646f0-3ddf-4550-d5a4-fe9fc52d875e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'studies' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-cac9467879ca>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m      \u001b[0;31m# TODO: Construye un DataFrame con los resultados\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m      \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstudies\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'studies' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Muestra los 10 primeros resultados\n"
      ],
      "metadata": {
        "id": "WJmIiDtpwEy8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#TODO: qué dimensiones tiene el dataset\n"
      ],
      "metadata": {
        "id": "560OiZy1wg-4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#TODO: qué datos contiene\n"
      ],
      "metadata": {
        "id": "rsrMEUVYwmbf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "La vectorización es el proceso por el que se convierte una colección de textos en un vector de características numéricas. El modelo que seguimos es el de bolsa de palabras o Bag-of-Words, donde los documentos se describen por las palabras que aparecen en el texto, ignorando su posición relativa o su importancia en el texto.\n",
        "\n",
        "CountVectorizer convierte una colección de documentos en una matriz de contadores que son las apariciones de cada token en cada documento.\n",
        "\n",
        "\n",
        "Empecemos con el campo 'BriefTitle', utilizando un vectorizador para convertir las palabras clave en una serie de elementos.\n"
      ],
      "metadata": {
        "id": "twOAIRs6rhTB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import pandas as pd\n",
        "\n",
        "# TODO: procesa el campo 'BriefTitle'\n",
        "\n"
      ],
      "metadata": {
        "id": "t0dhWcqLv2Ik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "¿Cuántas palabras distintas del título hay en el conjunto de datos?"
      ],
      "metadata": {
        "id": "qn5ere0WeuXb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#TODO\n"
      ],
      "metadata": {
        "id": "PkAawVXSx6Py"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO\n"
      ],
      "metadata": {
        "id": "RNK98zqMfIaS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Convertir en un DataFrame con 2 columnas: las diferentes palabras clave localizadas y su frecuencia de aparición:"
      ],
      "metadata": {
        "id": "pT-1b56ogo7p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#TODO:\n"
      ],
      "metadata": {
        "id": "5ptBPeJ3gpaU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Obtener las palabras clave que aparecen con mayor frecuencia:"
      ],
      "metadata": {
        "id": "CUji6jlrg_9q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#TODO:\n"
      ],
      "metadata": {
        "id": "XTwZ_p0-g7H6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Representar con un diagrama de barras ordenado las 5 palabras clave más frecuentes."
      ],
      "metadata": {
        "id": "ShmhRAZAhO-N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#TODO:\n"
      ],
      "metadata": {
        "id": "fQdvzNHnhP6y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ejercicio 5: distribución de frecuencias según la ley de Zipf\n",
        "\n",
        "La ley de Zipf es una observación empírica sobre la distribución de las frecuencias de las palabras en un lenguaje natural. Se basa en dos ideas principales:\n",
        "\n",
        "* Distribución inversa: La frecuencia de aparición de una palabra es inversamente proporcional a su rango en una lista ordenada de todas las palabras del texto. Esto significa que las palabras más frecuentes tienen un menor rango y viceversa.\n",
        "\n",
        "* Ley de los pequeños números: La frecuencia de una palabra es aproximadamente proporcional al inverso de su rango. En otras palabras, la frecuencia de la palabra es aproximadamente inversamente proporcional al rango de la palabra.\n",
        "\n",
        "La función de la Ley de Zipf relaciona el rango de una palabra ($r$) con su frecuencia de aparición ($f$) mediante la siguiente expresión:\n",
        "\n",
        "$f(r) = \\frac{C}{r^s}$\n",
        "\n",
        "dónde:\n",
        "$f(r)$ es la frecuencia de la palabra en el rango $r$.\n",
        "$C$ es una constante que depende del corpus de texto.\n",
        "$s$ es el exponente de la Ley de Zipf, que suele ser aproximadamente entre 0,7 y 1,0 para los textos en inglés.\n",
        "\n",
        "\n",
        "Esta función describe la relación entre el rango y la frecuencia de aparición de las palabras en un corpus de texto, siguiendo los principios de la Ley de Zipf."
      ],
      "metadata": {
        "id": "f0nAIaR7FFA3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para comprobar si es cierto que los textos siguen la ley de Zipf, importaremos la declaración de independencia de Estados Unidos, y comprobaremos si la frecuencia de las palabras cuadra con la frecuencia esperada por la ley de Zipf"
      ],
      "metadata": {
        "id": "DTZ0OT0joQlc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Subir archivo\n",
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "id": "uWMxoYsUomhQ",
        "outputId": "81d802a6-4b69-4842-8ac0-04a3e57b987f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-c918bf5c-e0bf-4967-83cf-aa494db362a5\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-c918bf5c-e0bf-4967-83cf-aa494db362a5\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving declaration-of-independence.txt to declaration-of-independence.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: importar archivo\n"
      ],
      "metadata": {
        "id": "sGjl30UnoyYi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#TODO: muestra el texto\n"
      ],
      "metadata": {
        "id": "8r45HiNSpIFV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#TODO: eliminar la puntuación\n",
        "\n"
      ],
      "metadata": {
        "id": "rD9qMhUvo8_g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#TODO:Obtener la frecuencia de las palabras ordenadas de más a menos\n",
        "from collections import Counter\n",
        "\n",
        "def top_freq_words(texto):\n",
        "     # TODO: convertir el texto a lower\n",
        "     # TODO: contar la frecuencia de las palabras\n",
        "     # TODO: ordenar la frecuencia en orden descendente\n",
        "     return sorted_word_freq"
      ],
      "metadata": {
        "id": "o3XO-G3FpcIW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: crear una dataframe con las frecuencias reales y las esperadas según la ley de Zipf\n",
        "def create_df_zip(sorted_word_freq):\n",
        "\n",
        "     returno df\n",
        "\n",
        "df_zip = create_df_zip(sorted_word_freq)"
      ],
      "metadata": {
        "id": "UVVJzXhSp7zd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#TODO: muestra el df"
      ],
      "metadata": {
        "id": "wl73VjNsqIcF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dibujar las frecuencias reales y las esperadas según la ley de Zipf\n"
      ],
      "metadata": {
        "id": "Azmj01RQqi_l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Comenta qué ves en el gráfico. ¿Qué pasaría si sacáramos las stopwords del texto?"
      ],
      "metadata": {
        "id": "yLpU0hSFq7c2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "# TODO: Saca las stop words en inglés\n"
      ],
      "metadata": {
        "id": "EgkVdzm9rB1c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: vuelve a mostrar la tabla df y el gráfico al excluir las stopwords\n"
      ],
      "metadata": {
        "id": "X5_LLc1FrUfb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Dibujar las frecuencias reales y las esperadas según la ley de Zipf (excluyendo las stopwords)\n"
      ],
      "metadata": {
        "id": "XB1ZuIFFrmDX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Comenta: ¿qué gráfico tiene más sentido?"
      ],
      "metadata": {
        "id": "6gTGOFVErtt_"
      }
    }
  ]
}