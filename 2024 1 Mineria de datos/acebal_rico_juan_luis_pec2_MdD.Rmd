---
title: 'Minería de datos: PEC2 - Métodos no supervisados'
author: "Autor: Juan Luis Acebal Rico"
date: "Noviembre 2024"
output:
  html_document:
    highlight: default
    number_sections: yes
    theme: cosmo
    toc: yes
    toc_depth: 2
    includes:
      in_header: 75.584-PEC-header.html
  pdf_document:
    highlight: zenburn
    toc: yes
  word_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

******
# Ejemplo guiado 1.1
## Método de agregación k-means con datos autogenerados
******
En este ejemplo vamos a generar un conjunto de muestras aleatorias para posteriormente usar el algoritmo *k-means* para agruparlas. Se crearán las muestras alrededor de dos puntos concretos. Por lo tanto, lo lógico será agrupar en dos clústers.  

Puesto que inicialmente, en un problema real, no se conoce cual es el número más idóneo de clústers k, vamos a probar primero con dos (el valor óptimo) y posteriormente con 4 y 8 clústers. Para evaluar la calidad de cada proceso de agrupación vamos a usar la silueta media. La silueta de cada muestra evalúa como de bien o mal está clasificada la muestra en el clúster al que ha sido asignada. Para ello se usa una fórmula que tiene en cuenta la distancia a las muestras de su clúster y la distancia a las muestras del clúster vecino más cercano. 

A la hora de probar el código que se muestra, es importante tener en cuenta que las muestras se generan de forma aleatoria y también que el algoritmo *k-means* tiene una inicialización aleatoria. Por lo tanto, en cada ejecución se obtendrá unos resultados ligeramente diferentes.  

Lo primero que hacemos es cargar la librería cluster que contiene las funciones que se necesitan  

```{r message= FALSE, warning=FALSE}
if (!require('cluster')) install.packages('cluster')
library(cluster)
```

Generamos las muestras de forma aleatoria tomando como centro los puntos [0,0] y [5,5].  

```{r message= FALSE, warning=FALSE}
n <- 150 # número de muestras
p <- 2   # dimensión

sigma <- 1 # varianza de la distribución
mean1 <- 0 # centro del primer grupo
mean2 <- 5 # centro del segundo grupo

n1 <- round(n/2) # número de muestras del primer grupo
n2 <- round(n/2) # número de muestras del segundo grupo

x1 <- matrix(rnorm(n1*p,mean=mean1,sd=sigma),n1,p)
x2 <- matrix(rnorm(n2*p,mean=mean2,sd=sigma),n2,p)
```

Juntamos todas las muestras generadas y las mostramos en una gráfica  

```{r message= FALSE, warning=FALSE}
x  <- rbind(x1,x2)
plot (x, xlab="Grupo 1", ylab="Grupo 2")
```

Como se puede comprobar las muestras están claramente separadas en dos grupos. Si se quiere complicar el problema se puede modificar los puntos centrales (mean1 y mean2) haciendo que estén más próximos y/o ampliar la varianza (sigma) para que las muestras estén más dispersas.  

A continuación vamos a aplicar el algoritmo *k-means* con 2, 4 y 8 clústers  

```{r message= FALSE, warning=FALSE}
# Función para ajustar k-means y mostrar los resultados
ajustar_kmeans <- function(k, vdata) {
  fit <- kmeans(vdata, centers = k)
  return(fit$cluster)
}

# Ajustos amb k = 2, 4, 8
clusters_2 <- ajustar_kmeans(2, x)
clusters_4 <- ajustar_kmeans(4, x)
clusters_8 <- ajustar_kmeans(8, x)

```

Las variables y_cluster2, y_cluster4 e y_cluster8 contienen para cada muestra el identificador del clúster a las que han sido asignadas. Por ejemplo, en el caso de los k=2 las muestras se han asignado al clúster 1 ó al 2  

```{r message= FALSE, warning=FALSE}
clusters_2 
```

Para visualizar los clústers podemos usar la función clusplot. Vemos la agrupación con 2 clústers y observemos como prácticamente no hay valores extremos y realmente los dos clústers generados son homogéneos.  

```{r message= FALSE, warning=FALSE}
clusplot(x, clusters_2, color=TRUE, shade=TRUE, labels=2, lines=0)
```

con 4 observamos como el clúster de la izquierda lo ha dividido en 3.  

```{r message= FALSE, warning=FALSE}
clusplot(x, clusters_4, color=TRUE, shade=TRUE, labels=2, lines=0)
```

y con 8. El algoritmo obedece y nos genera 8 clústers aunque como se aprecia visualmente no tenga demasiada consistencia.  

```{r message= FALSE, warning=FALSE}
clusplot(x, clusters_8, color=TRUE, shade=TRUE, labels=2, lines=0)
```

También podemos visualizar el resultado del proceso de agrupamiento con el siguiente código para el caso de 2 clústers. El uso de colores facilita la identificación visual de clústers.  

```{r message= FALSE, warning=FALSE}
# Función para graficar los resultados de los clusters
graficar_clusters <- function(vdata, y_cluster, k) {
  colors <- rainbow(k)
  plot(vdata, col = colors[y_cluster], xlab = "Dimensión 1", ylab = "Dimensión 2", 
       main = paste("Clusters con k =", k))
  legend("topright", legend = 1:k, fill = colors, title = "Clusters")
}

# Gráfico para k = 2
graficar_clusters(x, clusters_2, 2)  

```

para 4  

```{r message= FALSE, warning=FALSE}

# Gráfico para k = 4
graficar_clusters(x, clusters_4, 4)  

```

y para 8  

```{r message= FALSE, warning=FALSE}
# Gráfico para k = 8
graficar_clusters(x, clusters_8, 8)  

```

Ahora vamos a evaluar la calidad del proceso de agregación. Para ello usaremos la función silhouette que calcula la silueta de cada muestra  

```{r message= FALSE, warning=FALSE}
# Función para calcular y mostrar la silueta
calcular_silueta <- function(y_cluster, vdata) {
  distances <- daisy(vdata)
  silueta <- silhouette(y_cluster, distances)
  mean_sil <- mean(silueta[, 3])
  return(mean_sil)
}

sil_2 <- calcular_silueta(clusters_2, x)
sil_4 <- calcular_silueta(clusters_4, x)
sil_8 <- calcular_silueta(clusters_8, x)
```

La función silhouette devuelve para cada muestra, el clúster dónde ha sido asignado, el clúster vecino y el valor de la silueta. Por lo tanto, calculando la media de la tercera columna podemos obtener una estimación de la calidad del agrupamiento  

```{r message= FALSE, warning=FALSE}
# Mostrar valores de siluetas medianas
cat("Silueta mediana para k=2:", sil_2, "\n")
cat("Silueta mediana para k=4:", sil_4, "\n")
cat("Silueta mediana para k=8:", sil_8, "\n")
```

Como se puede comprobar, agrupar con dos clúster es mejor que en 4 o en 8, lo cual es lógico teniendo en cuenta como se han generado los datos. 

Una buena práctica para entender mejor el juego de datos, consiste en poner nombre a cada uno de los clústers identificados. Lo veremos más claramente en el siguiente ejemplo que parte de datos reales.

******
# Ejemplo guiado 1.2
## Método de agregación k-means con datos reales 
******

A continuación vamos a ver otro ejemplo de cómo se usan los modelos de agregación. Para ello usaremos el data set **penguins** contenido en el paquete R **palmerpenguins**. Esta base de datos se encuentra descrita en https://cran.r-project.org/web/packages/palmerpenguins/index.html y contiene  mediciones de tamaño, observaciones de puestas y proporciones de isótopos sanguíneos de tres especies de pingüinos observadas en tres islas del archipiélago Palmer, en la Antártida, durante un período de estudio de tres años.  

Este dataset está previamente trabajado para que los datos estén limpios y sin errores. De no ser así antes de nada deberíamos buscar errores, valores nulos u *outliers*. Deberíamos tratar de discretizar o eliminar columnas. Incluso realizar este último paso varias veces para comprobar los diferentes resultados y elegir el que mejor rendimiento nos dé. De todos modos contiene algún valor nulo que procederemos a ignorar.  

Vamos a visualizar la estructura y resumen de los datos  

```{r message= FALSE, warning=FALSE}
if (!require('palmerpenguins')) install.packages('palmerpenguins')
library(palmerpenguins)
palmerpenguins::penguins
summary(penguins)
```

Como se puede comprobar, esta base de datos está pensada para problemas de clasificación supervisada que pretende clasificar cada tipo de pingüino en una de las tres clases o especies existentes (Adelie, Gentoo o Chinstrap). Como en este ejemplo vamos a usar un método no supervisado, transformaremos el problema supervisado original en uno **no supervisado**. Para conseguirlo no usaremos la columna *species*, que es la variable que se quiere predecir. Por lo tanto, intentaremos encontrar agrupaciones usando únicamente los cuatro atributos numéricos que caracterizan a cada especie de pingüino.  
 x <- na.omit(penguins[,3:6])
Cargamos  los datos y nos quedamos únicamente con las cuatro columnas que definen a cada especie.    

```{r message= FALSE, warning=FALSE}
x_clean <- na.omit(penguins[,c(1,3,4,5,6)])
x_species <- x_clean[,1]
x <- x_clean[,2:5]
```

Planteamos ahora un ejemplo más realista en el que inicialmente no conocemos el número óptimo de clústers. Empecemos probamos con varios valores.  

```{r message= FALSE, warning=FALSE}
# Silhouette method
max_k <- 10
silhouettes <- numeric(max_k)
for (k in 2:max_k) {
  y_cluster <- kmeans(x, centers = k)$cluster
  silhouettes[k] <- calcular_silueta(y_cluster, x)
}
```

Mostramos en un gráfica los valores de las siluetas media de cada prueba para comprobar que número de clústers es el mejor.    

```{r message= FALSE, warning=FALSE}
plot(2:max_k, silhouettes[2:max_k], type = "b", pch = 19, frame = FALSE,
     xlab = "Número de clusters (k)", ylab = "Silhouette Score",
     main = "Silhouette para determinar el número óptimo de k")
```

Los valores de la silueta pueden fluctuar en el rango [-1,1], siendo valores cercanos a 1 indicativos de homogeneidad en los grupos y por el contrario valores de la silueta cercanos a -1 son indicativos de poca homogeneidad en los grupos, de modo que quisiéramos encontrarnos en un rango razonablemente cerca de 1.  

En el caso de nuestro juego de datos, a pesar de que uno esperaría obtener un valor óptimo para k = 3, parece que del gráfico se desprende que es mejor k = 2.   

Sin embargo, merece la pena observar que a partir de k = 3 la pérdida de homogeneidad es relativamente pequeña ya que se mantiene estable en el rango [0.50, 0.56]. Este hecho podría ser un argumento para seleccionar k = 3.  

Otra forma de evaluar cual es el mejor número de clústers es considerar el mejor modelo, aquel que ofrece la menor suma de los cuadrados de las distancias de los puntos de cada grupo con respecto a su centro (withinss), con la mayor separación entre centros de grupos (betweenss). Como se puede comprobar es una idea conceptualmente similar a la silueta. Una manera común de hacer la selección del número de clústers consiste en aplicar el método *elbow* (codo), que no es más que la selección del número de clústers en base a la inspección de la gráfica que se obtiene al iterar con el mismo conjunto de datos para distintos valores del número de clústers. Se seleccionará el valor que se encuentra en el "codo" de la curva.  

```{r message= FALSE, warning=FALSE}
# Función para calcular la inercia intracluster (Within-cluster sum of squares)
inercia_intracluster <- function(vdata, max_k) {
  wss <- numeric(max_k)
  for (k in 1:max_k) {
    wss[k] <- sum(kmeans(vdata, centers = k)$withinss)
  }
  return(wss)
}

# Elbow method
wss <- inercia_intracluster(x, max_k)
plot(1:max_k, wss, type = "b", pch = 19, frame = FALSE,
     xlab = "Número de clusters (k)", ylab = "Within-cluster sum of squares",
     main = "Elbow Method para determinar el número óptimo de k")

```

En este caso el número óptimo de clústers son 4 que es cuando la curva comienza a estabilizarse.  

También se puede usar la función *kmeansruns* del paquete **fpc** que ejecuta el algoritmo kmeans con un conjunto de valores, para después seleccionar el valor del número de clústers que mejor funcione de acuerdo a dos criterios: la silueta media ("asw") y *Calinski-Harabasz* ("ch").  

```{r message= FALSE, warning=FALSE}
if (!require('fpc')) install.packages('fpc')
library(fpc)
fit_ch  <- kmeansruns(x, krange = 1:10, criterion = "ch") 
fit_asw <- kmeansruns(x, krange = 1:10, criterion = "asw") 
```

Podemos comprobar el valor con el que se ha obtenido el mejor resultado y también mostrar el resultado obtenido para todos los valores de k usando ambos criterios  

```{r message= FALSE, warning=FALSE}
fit_ch$bestk
fit_asw$bestk

plot(1:10,fit_ch$crit,type="o",col="blue",pch=0,xlab="Número de clústers",ylab="Criterio Calinski-Harabasz")
plot(1:10,fit_asw$crit,type="o",col="blue",pch=0,xlab="Número de clústers",ylab="Criterio silueta media")
```

Los resultados son muy parecidos a los que hemos obtenido anteriormente. Con el criterio de la silueta media se obtienen dos clústers y con el *Calinski-Harabasz* se obtienen 3.  

Como se ha comprobado, conocer el número óptimo de clústers no es un problema fácil. Tampoco lo es la evaluación de los modelos de agregación.  

Como en el caso que estudiamos sabemos que los datos pueden ser agrupados en 3 clases o especies, vamos a ver cómo se ha comportado *kmeans* en el caso de pedirle 3 clústers. Para eso comparamos visualmente los campos dos a dos, con el valor real que sabemos está almacenado en el campo "species" del dataset original.  

(Aclaramos que obviamente no acostumbra a pasar que conozcamos de forma previa el número de clústers óptimo. Este ejemplo lo planteamos con finalidades didácticas y con voluntad de experimentar)  

```{r message= FALSE, warning=FALSE}
penguins3clusters <- kmeans(x, 3)

# bill_length y bill_depth
plot(x[c(1,2)], 
     col = penguins3clusters$cluster,    # Color by k-means cluster
     main = "Clasificación k-means",     # Plot title
     xlab = "bill_length",
     ylab = "bill_depth")

# Add a legend for the clusters
legend("topright", 
       legend = unique(penguins3clusters$cluster), # Cluster labels
       col = unique(penguins3clusters$cluster),    # Colors corresponding to clusters
       pch = 19,                                  # Point symbol in the legend
       title = "K-means Clusters")

# Plot with real species classification
plot(x[c(1,2)], 
     col = as.factor(x_species$species), # Color by real species
     main = "Clasificación real",        # Plot title
     xlab = "bill_length",
     ylab = "bill_depth")

# Add a legend for species
legend("topright", 
       legend = unique(x_species$species),        # Species labels
       col = unique(as.factor(x_species$species)), # Colors corresponding to species
       pch = 19,                                  # Point symbol in the legend
       title = "Species")
```

Podemos observar que *flipper_length* y *body_mass* no son buenos indicadores para diferenciar a las tres subespecies, dado que dos de las subespecies están demasiado mezcladas para poder diferenciar nada.  

```{r message= FALSE, warning=FALSE}
# flipper_length y body_mass
plot(x[c(3,4)], 
     col = penguins3clusters$cluster,    # Color by k-means cluster
     main = "Clasificación k-means",     # Plot title
     xlab = "flipper_length",
     ylab = "body_mass")

# Add a legend for the clusters
legend("topright", 
       legend = unique(penguins3clusters$cluster), # Cluster labels
       col = unique(penguins3clusters$cluster),    # Colors corresponding to clusters
       pch = 19,                                  # Point symbol in the legend
       title = "K-means Clusters")

# Plot with real species classification
plot(x[c(3,4)], 
     col = as.factor(x_species$species), # Color by real species
     main = "Clasificación real",        # Plot title
     xlab = "flipper_length",
     ylab = "body_mass")

# Add a legend for species
legend("topright", 
       legend = unique(x_species$species),        # Species labels
       col = unique(as.factor(x_species$species)), # Colors corresponding to species
       pch = 19,                                  # Point symbol in the legend
       title = "Species")
```

```{r message= FALSE, warning=FALSE}
# bill_length y flipper_length
plot(x[c(1,3)], 
     col = penguins3clusters$cluster,    # Color by k-means cluster
     main = "Clasificación k-means",     # Plot title
     xlab = "bill_length",          
     ylab = "flipper_length")

# Add a legend for the clusters
legend("topright", 
       legend = unique(penguins3clusters$cluster), # Cluster labels
       col = unique(penguins3clusters$cluster),    # Colors corresponding to clusters
       pch = 19,                                  # Point symbol in the legend
       title = "K-means Clusters")

# Plot with real species classification
plot(x[c(1,3)], 
     col = as.factor(x_species$species), # Color by real species
     main = "Clasificación real",        # Plot title
     xlab = "bill_length",          
     ylab = "flipper_length")           
     
# Add a legend for species
legend("topright", 
       legend = unique(x_species$species),        # Species labels
       col = unique(as.factor(x_species$species)), # Colors corresponding to species
       pch = 19,                                  # Point symbol in the legend
       title = "Species")

```

Las dos medidas de *bill* parecen lograr mejores resultados al dividir las tres especies de pingüinos. El grupo formado por los puntos negros que ha encontrado el algoritmo coincide con los de la especie *Adelie*. Los otros dos grupos sin embargo se entremezclan algo más, y hay ciertos puntos que se clasifican como *Gentoo* (verde) cuando en realidad son *Chinstrap* (rojo).  
 
 Una buena técnica que ayuda a entender los grupos que se han formado, es mirar de darles un nombre. Cómo por ejemplo:  
 
 - Grupo 1: Sólo *Adelie* (color negro)  
 - Grupo 2: Principalmente *Chinstrap* (color rojo)  
 - Grupo 3: Mezcla de *Gentoo* (color verde) y *Adelie* (color negro)  
 
Esto nos ayuda a entender cómo están formados los grupos y a referirnos a ellos en análisis posteriores.  
 
Todo esto nos indica que el número de grupos o clúsers en un juego de datos no es un aspecto que podamos asegurar que siempre vamos a encontrar de forma precisa y objetiva, bien al contrario es un ámbito que requiere de análisis en sí mismo.  

Os compartimos en el siguiente enlace un material didáctico complementario que os puede ayudar a profundizar en el tema de la selección del número de clústers más adecuado para un juego de datos:  
<a href="http://datascience.recursos.uoc.edu/es/como-podemos-elegir-el-numero-de-clusteres" target="_blank">datascience.recursos.uoc.edu</a>

Como continuación del estudio podríamos seguir experimentando combinando en gráficos similares a los anteriores. En definitiva se trataría en este punto de profundizar más en el conocimiento de las propiedades de las diferentes características o columnas del juego de datos.    

******
# Ejemplo guiado 2
## Métodos basados en densidad: DBSCAN y OPTICS
******
En este ejemplo vamos ha trabajar los algoritmos **DBSCAN** y **OPTICS** como métodos de clustering que permiten la generación de grupos no radiales a diferencia de k-means. Veremos que su parámetro de entrada más relevante es *minPts* que define la mínima densidad aceptada alrededor de un centroide.  

Incrementar este parámetro nos permitirá reducir el ruido (observaciones no asignadas a ningún cluster), en cualquier caso empezaremos por construir nuestro propio juego de datos en el que dibujaremos 4 zonas de puntos diferenciadas.  


```{r message= FALSE, warning=FALSE}
if (!require('dbscan')) install.packages('dbscan')
library(dbscan)
set.seed(2)
n <- 400
x <- cbind(
x = runif(4, 0, 1) + rnorm(n, sd=0.1),
y = runif(4, 0, 1) + rnorm(n, sd=0.1)
)
plot(x, col=rep(1:4, time = 100))
```


Una de las primeras actividades que realiza el algoritmo es **ordenar las observaciones** de forma que los puntos más cercanos se conviertan en vecinos en el ordenamiento. Se podría pensar como una representación numérica del dendograma de una agrupación jerárquica.  

```{r message= FALSE, warning=FALSE}
### Lanzamos el algoritmo OPTICS dejando el parámetro eps con su valor por defecto y fijando el criterio de vecindad en 10
res <- optics(x, minPts = 10)
res
### Obtenemos la ordenación de las observaciones o puntos
res$order

```

Otro paso muy interesante del algoritmo es la generación de un **diagrama de alcanzabilidad** o *reachability plot,* en el que se aprecia de una forma visual la distancia de alcanzabilidad de cada punto.  

Los valles representan clusters (cuanto más profundo es el valle, más denso es el cluster), mientras que las cimas indican los puntos que están entre las agrupaciones (estos puntos son candidatos a ser considerados *outliers*)  

```{r message= FALSE, warning=FALSE}
### Gráfica de alcanzabilidad
plot(res)
```
  
  
Veamos otra representación del diagrama de alcanzabilidad, donde podemos observar las trazas de las distancias entre puntos cercanos del mismo cluster y entre clusters distintos.  


```{r message= FALSE, warning=FALSE}
### Dibujo de las trazas que relacionan puntos
plot(x, col = "grey")
polygon(x[res$order,])
```


Otro ejercicio interesante a realizar es extraer una agrupación de la ordenación realizada por OPTICS similar a lo que DBSCAN hubiera generado estableciendo el parámetro eps en eps_cl = 0.065. En este sentido animamos al estudiante a experimentar con diferentes valores de este parámetro.   


```{r message= FALSE, warning=FALSE}
### Extracción de un clustering DBSCAN cortando la alcanzabilidad en el valor eps_cl
res <- extractDBSCAN(res, eps_cl = .065)
res
plot(res) ## negro indica ruido
```

Observamos en el gráfico anterior como se han coloreado los 4 clusters y en negro se mantienen los valores *outliers* o extremos.    

Seguimos adelante con una representación gráfica que nos muestra los clusters mediante formas convexas.  


```{r message= FALSE, warning=FALSE}
hullplot(x, res)
```
  
Repetimos el experimento anterior incrementando el parámetro *epc_c*, veamos como el efecto que produce es la concentración de clusters ya que flexibilizamos la condición de densidad.  


```{r message= FALSE, warning=FALSE}
### Incrementamos el parámetro eps
res <- extractDBSCAN(res, eps_cl = .1)
res
plot(res)
hullplot(x, res)
```


Veamos ahora una variante de la extracción **DBSCN** anterior. En ella el parámetro *xi* nos va a servir para clasificar los clusters en función del cambio en la densidad relativa de los mismos.  


```{r message= FALSE, warning=FALSE}
### Extracción del clustering jerárquico en función de la variación de la densidad por el método xi
res <- extractXi(res, xi = 0.05)
res
plot(res)
x
```

# Ejercicios
Los ejercicios se realizarán en base al juego de datos *Hawks* presente en el paquete R *Stat2Data*.  

Los estudiantes y el profesorado del Cornell College en Mount Vernon, Iowa, recogieron datos durante muchos años en el mirador de halcones del lago MacBride, cerca de Iowa City, en el estado de Iowa. El conjunto de datos que analizamos aquí es un subconjunto del conjunto de datos original, utilizando sólo aquellas especies para las que había más de 10 observaciones. Los datos se recogieron en muestras aleatorias de tres especies diferentes de halcones: Colirrojo, Gavilán y Halcón de Cooper.  

Hemos seleccionado este juego de datos por su parecido con el juego de datos *penguins* y por su potencial a la hora de aplicarle algoritmos de minería de datos no supervisados. 


```{r message= FALSE, warning=FALSE}
if (!require('Stat2Data')) install.packages('Stat2Data')
library(Stat2Data)
data("Hawks")
summary(Hawks)
```


## Ejercicio 1
Presenta el juego de datos, nombre y significado de cada columna, así como las distribuciones de sus valores.  

Realiza un estudio aplicando el método K-means, similar al de los ejemplos 1.1 y 1.2   

### Respuesta 1

```{r, out.width="80%", fig.align='center'}

data("Hawks")

# Exploramos la estructura del conjunto de datos
str(Hawks)
```
#### Explicación, análisis y descripción del dataset

El dataset es, tal y como se ha explicado en el enunciado, de un mirador de halcones. Son observaciones de estos entre 1992 y 2003.

Además son 908 registros y 19 variables, de las cuales 2 están casi vacías (Tarsus y WingPitFat) y 4 están con una cantidad importante con valores faltantes (Sex, StandardTail, KeelFat y Crop ).

El motivo de que haya tantos valores faltantes puede ser de la diferencia de tiempo (11 años) en los cuales los criterios de regocida de datos hayan cambiado, así como dificultades para tomar el valor para algunos halcones, etc. Sería interesante ver si cambió el criterio o es aleatorio.

En líneas generales, el diccionario de datos del dataset es:

Month: Numérica, mes de la captura, siendo 8,9, 10 u 11 los meses que se hicieron capturas.

Day: Numérica, día en el que se capturó (entero del 1 al 31)

Year: Numérica, año de la captura

CaptureTime: Categórica, hora de captura

ReleaseTime: Categórica, hora de liberación

BandNumber: Categórica, ID de la banda asignada a cada halcón

Species: Categórica, especie del halcón; CH, RT, SS

Age: Categórica, edad del halcón; A (adult), I (inmaduro, inmmadure)

Sex: Categórica, sexo; F,M

Wing: Numérica, longitud alasen cm

Weight: Numérica, peso en gm

Hallux: Numérica, longitud de la garra

Culmen: Numérica, longitud del pico

Tail: Numérica, longitud de la cola

StandardTail: Numérica, medida alternativa a Tail con otra manera de medicción. Hay pocos registros que tienen valores.

Tarsus: Numérica, longitud del tarso

WingPitFat: Numérica, grasa acomulada en el ala

KeelFat: Numérica, grasa acomulada en el esternón

Crop: Numérica, cantidad de alimento en el buche




#### Distribucion de las variables. Histogramas 

```{r, out.width="80%", fig.align='center'}
par(mfrow = c(2, 2)) 
hist(Hawks$Wing, main = "Histograma de Wing", xlab =  "Wing (mm)" )
hist(Hawks$Weight, main = "Histograma de Weight", xlab = "Weight (g)" )
hist(Hawks$Culmen, main = "Histograma de Culmen", xlab = "Culmen (mm)")
hist(Hawks$Tail, main = "Distribución de Tail", xlab = "Tail (mm)")
#Hallux tiene un gran outlier, entonces tengo que poner muchos bins o breaks para que se vea la distribución
hist(Hawks$Hallux, main = "Histograma de Hallux", xlab = "Hallux (mm)", breaks = 100)


```
Bueno aqui vemos la distribución de los distintos valores, podemos ver que, no hay un patrón claro, y lo más llamativo es los outliers en Hallux.


#### Box plot by especie. 


```{r, out.width="80%", fig.align='center'}
boxplot(Wing ~ Species, data = Hawks, main = "Distribución de Wing por Especie", 
        ylab = "Wing (mm)", xlab = "Especie", col = c("cyan", "brown", "yellow"))

boxplot(Weight ~ Species, data = Hawks, main = "Distribución de Weight por Especie", 
        ylab = "Weight (g)", xlab = "Especie", col = c("cyan", "brown", "yellow"))
```
```{r, out.width="80%", fig.align='center'}
boxplot(Culmen ~ Species, data = Hawks, main = "Distribución de Culmen por Especie", 
        ylab = "Culmen (mm)", xlab = "Especie", col = c("cyan", "brown", "yellow"))

boxplot(Hallux ~ Species, data = Hawks, main = "Distribución de Hallux por Especie", 
        ylab = "Hallux (mm)", xlab = "Especie", col = c("cyan", "brown", "yellow"))
```
##### Box plots y summary para las especies CH y SS

```{r, out.width="80%", fig.align='center'}
hawks_subset <- subset(Hawks, Species %in% c("CH"))
summary(hawks_subset)
```

```{r, out.width="80%", fig.align='center'}
hawks_subset <- subset(Hawks, Species %in% c("SS"))
summary(hawks_subset)
```



```{r, out.width="80%", fig.align='center'}
hawks_subset <- subset(Hawks, Species %in% c("CH", "SS"))
summary(hawks_subset)
boxplot(Wing ~ Species, data = hawks_subset, main = "Distribución de Wing por Especie", 
        ylab = "Wing (mm)", xlab = "Especie", col = c("cyan", "brown", "yellow"))
```
```{r, out.width="80%", fig.align='center'}
boxplot(Weight ~ Species, data = hawks_subset, main = "Distribución de Weight por Especie", 
        ylab = "Weight (g)", xlab = "Especie", col = c("cyan", "brown", "yellow"))

boxplot(Culmen ~ Species, data = hawks_subset, main = "Distribución de Culmen por Especie", 
        ylab = "Culmen (mm)", xlab = "Especie", col = c("cyan", "brown", "yellow"))
```

```{r, out.width="80%", fig.align='center'}
boxplot(Hallux ~ Species, data = hawks_subset, main = "Distribución de Hallux por Especie", 
        ylab = "Hallux (mm)", xlab = "Especie", col = c("cyan", "brown", "yellow"))

boxplot(KeelFat ~ Species, data = hawks_subset, main = "Distribución de KeelFat por Especie", 
        ylab = "KeelFat", xlab = "Especie", col = c("cyan", "brown", "yellow"))

```

```{r, out.width="80%", fig.align='center'}
boxplot(StandardTail ~ Species, data = hawks_subset, main = "Distribución de StandardTail por Especie", 
        ylab = "StandardTail (mm)", xlab = "Especie", col = c("cyan", "brown", "yellow"))

boxplot(Crop ~ Species, data = hawks_subset, main = "Distribución de Crop por Especie", 
        ylab = "Crop (mm)", xlab = "Especie", col = c("cyan", "brown", "yellow"))
```


Aquí podemos observar, que hay una clara distribución y concentración de valores según la escecie que se trate, algo evidente y bastante interesante para análisis posteriores. Ademas si separamos RT de CH y SS, y ponemos la lupa en esas dos, podemos ver como Wing no ayudará suficiente para separar CH de SS, ya que sus bigotes del boxplot terminan en la caja de este. Si bien aquí podríamos llegar a casi separar todos los grupos, el análisis se puede volver muy largo ya que el siguiente paso que haría sería ver si edad afecta a las variables.


###### Boxplot de CH y SS filtrando por adultos

```{r, out.width="80%", fig.align='center'}
hawks_subset <- subset(hawks_subset, Age %in% c("A"))
summary(hawks_subset)
boxplot(Wing ~ Species, data = hawks_subset, main = "Distribución de Wing por Especie", 
        ylab = "Wing (mm)", xlab = "Especie", col = c("cyan", "brown", "yellow"))
```
```{r, out.width="80%", fig.align='center'}
boxplot(Weight ~ Species, data = hawks_subset, main = "Distribución de Weight por Especie", 
        ylab = "Weight (g)", xlab = "Especie", col = c("cyan", "brown", "yellow"))

boxplot(Culmen ~ Species, data = hawks_subset, main = "Distribución de Culmen por Especie", 
        ylab = "Culmen (mm)", xlab = "Especie", col = c("cyan", "brown", "yellow"))
```



```{r, out.width="80%", fig.align='center'}
boxplot(Hallux ~ Species, data = hawks_subset, main = "Distribución de Hallux por Especie", 
        ylab = "Hallux (mm)", xlab = "Especie", col = c("cyan", "brown", "yellow"))

boxplot(KeelFat ~ Species, data = hawks_subset, main = "Distribución de KeelFat por Especie", 
        ylab = "KeelFat", xlab = "Especie", col = c("cyan", "brown", "yellow"))

```


```{r, out.width="80%", fig.align='center'}
boxplot(StandardTail ~ Species, data = hawks_subset, main = "Distribución de StandardTail por Especie", 
        ylab = "StandardTail (mm)", xlab = "Especie", col = c("cyan", "brown", "yellow"))

boxplot(Crop ~ Species, data = hawks_subset, main = "Distribución de Crop por Especie", 
        ylab = "Crop (mm)", xlab = "Especie", col = c("cyan", "brown", "yellow"))
```


Si bien para este análisis no nos sirve, ya que perderíamos una parte importante del dataset, mas de 200 individuos, es interesante saber que, en los adultos podemos tener identificadas las 3 especies, sea por kmeans u otro método, ya que tenemos bien acotadas las medidas en Wing y StandardTail para diferenciar estas 2 especies, sumado al análisis exploratorio que ya he hecho de RT, puedo llegar a la conclusión que podría hacer un modelo eficiente para adultos que clasifique según medidas.



#### Box plot by Sex.


```{r, out.width="80%", fig.align='center'}
boxplot(Wing ~ Sex, data = Hawks, main = "Distribución de Wing por Sexo", 
        ylab = "Wing (mm)", xlab = "Sexo", col = c("cyan", "brown", "yellow"))

boxplot(Weight ~ Sex, data = Hawks, main = "Distribución de Weight por Sexo", 
        ylab = "Weight (g)", xlab = "Sexo", col = c("cyan", "brown", "yellow"))

```
```{r, out.width="80%", fig.align='center'}
boxplot(Culmen ~ Sex, data = Hawks, main = "Distribución de Culmen por Sexo", 
        ylab = "Culmen (mm)", xlab = "Sexo", col = c("cyan", "brown", "yellow"))

boxplot(Hallux ~ Sex, data = Hawks, main = "Distribución de Hallux por Sexo", 
        ylab = "Hallux (mm)", xlab = "Sexo", col = c("cyan", "brown", "yellow"))
```


Sin embago si es por sexo, la distribución no está clara. ¿Y si en ciertas especies es más complicado saber el sexo? No tiene sentido que los individuos con sexo conocido se sitúen en una zona concreta y los que no tienen sexo conocido, en otra totalmente diferente.

#### Boxplot by Especie y Sex en Wing


```{r, out.width="80%", fig.align='center'}
boxplot(Wing ~ Species, data = subset(Hawks, Sex == "M"), 
        main = "Distribución de Wing por Especie (Sexo = M)", 
        ylab = "Wing (mm)", xlab = "Especie", col = c("cyan", "brown", "yellow"))

```
```{r, out.width="80%", fig.align='center'}

boxplot(Wing ~ Species, data = subset(Hawks, Sex == "F"), 
        main = "Distribución de Wing por Especie (Sexo = F)", 
        ylab = "Wing (mm)", xlab = "Especie", col = c("cyan", "brown", "yellow"))
```




```{r, out.width="80%", fig.align='center'}
boxplot(Wing ~ Species, data = subset(Hawks, Sex == ""), 
        main = "Distribución de Wing por Especie (Sexo = Nulo)", 
        ylab = "Wing (mm)", xlab = "Especie", col = c("cyan", "brown", "yellow"))

```






```{r, out.width="80%", fig.align='center'}
nulos_sex=subset(Hawks, Sex == "")
conteo_sex <- table(nulos_sex$Sex)
conteo_sex
conteo_especies <- table(nulos_sex$Species)
conteo_especies

```



```{r, out.width="80%", fig.align='center'}
species_solo_f <- subset(Hawks, Sex == "F" )
species_solo_f <- table(species_solo_f$Species)
species_solo_f
species_solo_m <- subset(Hawks, Sex == "M" )
species_solo_m <- table(species_solo_m$Species)
species_solo_m
```
Bueno, aquí parece que tenemos dos conclusiones, en CH y SS tiene una distribución normal de Sex, sin embargo, en RT, no tenemos Sex, por tanto no podemos usar la especie RT si conlleva utilizar Sex, ya que ni siquiera podríamos hacer imputación de valores tomando valores cercanos ni nada por el estilo ya que no hay apenas muestras en la especie RT...

#### Métodos de búsqueda de valores óptimos de clusters

##### Limpieza de nulos y escalado

```{r, out.width="80%", fig.align='center'}
hawks_limpio <- Hawks[, c("Wing", "Weight", "Culmen", "Tail")]#, "Hallux")]
hawks_limpio <- na.omit(hawks_limpio)
hawks_scaled <- scale(hawks_limpio)

```

##### Métodos de cálculo de valor óptimo de número de clústers

```{r, out.width="80%", fig.align='center'}
max_k <- 10
silhouettes <- numeric(max_k)
for (k in 2:max_k) {
  y_cluster <- kmeans(hawks_scaled, centers = k)$cluster
  silhouettes[k] <- calcular_silueta(y_cluster, hawks_scaled)
}
plot(2:max_k, silhouettes[2:max_k], type = "b", pch = 19, frame = FALSE,
     xlab = "Número de clusters (k)", ylab = "Silhouette Score",
     main = "Silhouette para determinar el número óptimo de k")
inercia_intracluster <- function(vdata, max_k) {
  wss <- numeric(max_k)
  for (k in 1:max_k) {
    wss[k] <- sum(kmeans(vdata, centers = k)$withinss)
  }
  return(wss)
}
wss <- inercia_intracluster(hawks_scaled, max_k)

```


```{r, out.width="80%", fig.align='center'}
plot(1:max_k, wss, type = "b", pch = 19, frame = FALSE,
     xlab = "Número de clusters (k)", ylab = "Within-cluster sum of squares",
     main = "Elbow Method para determinar el número óptimo de k")

```

Vemos que en ambos es 2... y en el primero también 3...

##### Visualización de 2 y 3 clusters

```{r, out.width="80%", fig.align='center'}
usar_kmeans <- function(k, vdata) {
  fit <- kmeans(vdata, centers = k)
  return(fit$cluster)
}
clusters_02 <- usar_kmeans(2,hawks_scaled)
clusters_03 <- usar_kmeans(3,hawks_scaled)

clusplot(hawks_scaled, clusters_02, color=TRUE, shade=TRUE, labels=2, lines=0, 
         main="Clusplot: 2 Clusters")



```


```{r, out.width="80%", fig.align='center'}
clusplot(hawks_scaled, clusters_03, color=TRUE, shade=TRUE, labels=2, lines=0, 
         main="Clusplot: 3 Clusters")
Hawks$Cluster_A_2 <- NA
Hawks$Cluster_A_3 <- NA
Hawks[rownames(hawks_scaled), "Cluster_A_2"] <- clusters_02
Hawks[rownames(hawks_scaled), "Cluster_A_3"] <- clusters_03


```



##### Vista rápida con menos variables (2 en vez de 5)

```{r, out.width="80%", fig.align='center'}
hawks_scaled_menos_variables <-Hawks[ c("Wing", "Weight")]#, "Culmen", "Tail")]#, "Hallux")]
hawks_scaled_menos_variables <- na.omit(hawks_scaled_menos_variables)  
hawks_scaled_menos_variables <- scale(hawks_scaled_menos_variables)

max_k <- 10
silhouettes <- numeric(max_k)
for (k in 2:max_k) {
  y_cluster <- kmeans(hawks_scaled_menos_variables, centers = k)$cluster
  silhouettes[k] <- calcular_silueta(y_cluster, hawks_scaled_menos_variables)
}
plot(2:max_k, silhouettes[2:max_k], type = "b", pch = 19, frame = FALSE,
     xlab = "Número de clusters (k)", ylab = "Silhouette Score",
     main = "Silhouette para determinar el número óptimo de k")
inercia_intracluster <- function(vdata, max_k) {
  wss <- numeric(max_k)
  for (k in 1:max_k) {
    wss[k] <- sum(kmeans(vdata, centers = k)$withinss)
  }
  return(wss)
}
wss <- inercia_intracluster(hawks_scaled_menos_variables, max_k)



```


```{r, out.width="80%", fig.align='center'}
plot(1:max_k, wss, type = "b", pch = 19, frame = FALSE,
     xlab = "Número de clusters (k)", ylab = "Within-cluster sum of squares",
     main = "Elbow Method para determinar el número óptimo de k")


clusters_2 <- usar_kmeans(2, hawks_scaled_menos_variables)
clusters_3 <- usar_kmeans(3, hawks_scaled_menos_variables)

```

```{r, out.width="80%", fig.align='center'}
clusplot(hawks_scaled_menos_variables, clusters_2, color=TRUE,
         shade=TRUE, labels=2, lines=0, 
         main="Clusplot: 2 Clusters")

```




```{r, out.width="80%", fig.align='center'}
clusplot(hawks_scaled_menos_variables, clusters_3, color=TRUE,
         shade=TRUE, labels=2, lines=0, 
         main="Clusplot: 3 Clusters")
Hawks$Cluster_B_2 <- NA
Hawks$Cluster_B_3 <- NA
Hawks[rownames(hawks_scaled_menos_variables), "Cluster_B_2"] <- clusters_2
Hawks[rownames(hawks_scaled_menos_variables), "Cluster_B_3"] <- clusters_3


```





##### Vista rápida con menos variables( 2 en vez de 5) y solo adultos

```{r, out.width="80%", fig.align='center'}
Hawks_adultos<-subset(Hawks, Age %in% c("A"))

hawks_scaled_menos_variables <-Hawks_adultos[ c("Weight", "Wing") ]
hawks_scaled_menos_variables <- na.omit(hawks_scaled_menos_variables)  
hawks_scaled_menos_variables <- scale(hawks_scaled_menos_variables)

max_k <- 10
silhouettes <- numeric(max_k)
for (k in 2:max_k) {
  y_cluster <- kmeans(hawks_scaled_menos_variables, centers = k)$cluster
  silhouettes[k] <- calcular_silueta(y_cluster, hawks_scaled_menos_variables)
}
plot(2:max_k, silhouettes[2:max_k], type = "b", pch = 19, frame = FALSE,
     xlab = "Número de clusters (k)", ylab = "Silhouette Score",
     main = "Silhouette para determinar el número óptimo de k")
inercia_intracluster <- function(vdata, max_k) {
  wss <- numeric(max_k)
  for (k in 1:max_k) {
    wss[k] <- sum(kmeans(vdata, centers = k)$withinss)
  }
  return(wss)
}
wss <- inercia_intracluster(hawks_scaled_menos_variables, max_k)
plot(1:max_k, wss, type = "b", pch = 19, frame = FALSE,
     xlab = "Número de clusters (k)", ylab = "Within-cluster sum of squares",
     main = "Elbow Method para determinar el número óptimo de k")

clusters_2 <- usar_kmeans(2, hawks_scaled_menos_variables)
clusters_3 <- usar_kmeans(3, hawks_scaled_menos_variables)
clusplot(hawks_scaled_menos_variables, clusters_2, color=TRUE,
         shade=TRUE, labels=2, lines=0, 
         main="Clusplot: 2 Clusters")


```

```{r, out.width="80%", fig.align='center'}
clusplot(hawks_scaled_menos_variables, clusters_3, color=TRUE,
         shade=TRUE, labels=2, lines=0, 
         main="Clusplot: 3 Clusters")

Hawks$Cluster_C_2 <- NA
Hawks$Cluster_C_3 <- NA
Hawks[rownames(hawks_scaled_menos_variables), "Cluster_C_2"] <- clusters_2
Hawks[rownames(hawks_scaled_menos_variables), "Cluster_C_3"] <- clusters_3


```



Podemos observar que hay una diferencia significativa y que podríamos hacer un modelo de clasificación de adultos para las 3 especies casi con seguridad.

###### Usar kmeansruns
Podemos usar kmeans runs para buscar el numero optimo de clusters, si bien el uso es normalmente antes de clusterizar, lo he querido dejar plasmado para comparar con como lo he hecho, usando los dos criterios utilizados en el ejemplo, silueta media y Calinski-Harabasz, aunque la conclusión hubiera sido parecida...

```{r message= FALSE, warning=FALSE}
fit_ch  <- kmeansruns(hawks_scaled, krange = 1:10, criterion = "ch") 
fit_asw <- kmeansruns(hawks_scaled, krange = 1:10, criterion = "asw") 
fit_ch$bestk
fit_asw$bestk
plot(1:10,fit_ch$crit,type="o",col="blue",pch=0,xlab="Número de clústers",
     ylab="Criterio Calinski-Harabasz")
```

```{r, out.width="80%", fig.align='center'}
plot(1:10,fit_asw$crit,type="o",col="blue",pch=0,xlab="Número de clústers",
     ylab="Criterio silueta media")

```



###### Ver el dataset original


```{r, out.width="80%", fig.align='center'}
head(Hawks)
```

###### Caracteristicas de cada cluster

```{r, out.width="80%", fig.align='center'}
# Datos agregados ponderados de cada cluster
aggregate(. ~ Cluster_A_2, data = Hawks[, c("Wing",
                                            "Weight",
                                            "Culmen",
                                            "Tail",
                                            "Hallux",
                                            "Cluster_A_2")], mean)

aggregate(. ~ Cluster_A_3, data = Hawks[, c("Wing", 
                                            "Weight",
                                            "Culmen",
                                            "Tail",
                                            "Hallux",
                                            "Cluster_A_3")], mean)

aggregate(. ~ Cluster_B_2, data = Hawks[, c("Wing",
                                            "Weight",
                                            "Culmen",
                                            "Tail",
                                            "Hallux",
                                            "Cluster_B_2")], mean)

aggregate(. ~ Cluster_B_3, data = Hawks[, c("Wing", "Weight",
                                            "Culmen", "Tail",
                                            "Hallux", "Cluster_B_3")], mean)

aggregate(. ~ Cluster_C_2, data = Hawks[, c("Wing", "Weight",
                                            "Culmen", "Tail",
                                            "Hallux", "Cluster_C_2")], mean)

aggregate(. ~ Cluster_C_3, data = Hawks[, c("Wing", "Weight",
                                            "Culmen", "Tail", "Hallux",
                                            "Cluster_C_3")], mean)


```
De los modelos probados, voy a comentar el A, he hecho el promedio, el cluster A para 2 clusters tiene claramente diferenciado las alas, el peso, etc, siendo unos halcones mucho mas grandes(1)que los otros(2). Si elijo el modelo con 3 clusters, se ve como tenemos uno con medidas muy grandes(1), otro con grandes(3), y otro con medidas pequeñas(2)



```{r, out.width="80%", fig.align='center'}

# Conteo de registros por especie y cluster
conteo_A_3 <- as.data.frame(table(Hawks$Species, Hawks$Cluster_A_3))
colnames(conteo_A_3) <- c("Species", "Cluster_A_3", "Conteo_A_3")

conteo_A_2 <- as.data.frame(table(Hawks$Species, Hawks$Cluster_A_2))
colnames(conteo_A_2) <- c("Species", "Cluster_A_2", "Conteo_A_2")

conteo_combinado <- merge(conteo_A_2, conteo_A_3, by = "Species", all = TRUE)
conteo_combinado
```

```{r, out.width="80%", fig.align='center'}
plot_clusters <- function(data, cluster_column) {
  
  data_no_na <- subset(data, !is.na(data[[cluster_column]]))
  
  data_no_na$Species_numeric <- ifelse(data_no_na$Species == "RT", 1,
                                       ifelse(data_no_na$Species == "SS", 2,
                                              ifelse(data_no_na$Species == "CH", 3, NA)))
  
  plot(data_no_na$Wing, data_no_na$Weight, 
       col = data_no_na[[cluster_column]],    
       pch = data_no_na$Species_numeric, 
       main = paste("Clasificación", cluster_column, "en Hawks"),     
       xlab = "Wing (mm)", 
       ylab = "Weight (g)",
       cex = 0.6)
  
  legend("topright", 
         legend = unique(data_no_na[[cluster_column]]), 
         col = unique(data_no_na[[cluster_column]]), 
         pch = 19,                    
         title = cluster_column)
  
  legend("bottomright",
         legend = c("RT", "SS", "CH"), 
         pch = c(1, 2, 3),             
         title = "Especies")
}

plot_clusters(Hawks, "Cluster_A_2")
plot_clusters(Hawks, "Cluster_A_3")
plot_clusters(Hawks, "Cluster_B_2")
plot_clusters(Hawks, "Cluster_B_3")
plot_clusters(Hawks, "Cluster_C_2")
plot_clusters(Hawks, "Cluster_C_3")
```

A excepcion de la pruebas de clusters A, B y C, con 3 clusters, el resto se asemeja bastante a las especies, aunque hay errores de asignación. Hay que tener en cuenta que en algunas ejecuciones me ha separado casi correctamente para algunos de los intentos de clusterización con 3 clusters, ya que la iniciacion de los centroides es aleatoria.



## Ejercicio 2
Con el juego de datos proporcionado realiza un estudio aplicando DBSCAN y OPTICS, similar al del ejemplo 2  

### Respuesta 2


```{r, out.width="80%", fig.align='center'}
hawks_num_cluster <- na.omit(Hawks[, c("Wing", "Weight","Cluster_A_3")])
hawks_num <-hawks_num_cluster[, c("Wing", "Weight")]

hawks_num_scaled <- scale(hawks_num)
optics_result_scaled <- optics(hawks_num_scaled, minPts = 5)
plot(optics_result_scaled, main = "Gráfico de alcanzabilidad OPTICS (Escalado)")
dbscan_extract_scaled <- extractDBSCAN(optics_result_scaled, eps_cl = 0.2)

plot(hawks_num_scaled[, 1], hawks_num_scaled[, 2], 
     col = dbscan_extract_scaled$cluster + 1,
     main = "Resultados del Clustering DBSCAN en el Dataset Hawks (Escalado)",
     xlab = "Wing (Escalado)", ylab = "Weight (Escalado)", pch = 19)

legend("topright", legend = unique(dbscan_extract_scaled$cluster),
       col = unique(dbscan_extract_scaled$cluster + 1), pch = 19, title = "Clusters DBSCAN")

optics_result <- extractXi(optics_result_scaled, xi = 0.05)
plot(optics_result)
```


Si vemos el reachability plot o gráfico de alcanzabilidad en español, hay unos picos muy altos, que representan la densidad, puesto que DBSCAN se basa en la densidad, y si hay clusters con diferentes densidades, quizás no lo detecte bien. Como vemos aquí, en general hay una densidad alta, con picos, que son outliers en muchos casos, estos picos representan zonas de baja densidad.

DBSCAN crea clusters en base a la densidad con dos variables fundamentales, Eps y MinPts. Eps representa la distancia maxima entre puntos vecinos y MinPts el numero minimo de puntos para formar un cluster.. dicho esto, cuando se utiliza DBSCAN hay que hacer atención a configurar la densidad adecuadamente, igual que en Kmeans se ce crean los centroides y luego se asignan los valores a su centroide mas cercano, generando esferas circulares.



## Ejercicio 3
Realiza una comparativa de los métodos *k-means* y *DBSCAN*    

```{r, out.width="80%", fig.align='center'}
hullplot(hawks_num_scaled, dbscan_extract_scaled$cluster, main = "Clusters con dbscan")
hullplot(hawks_num_scaled,hawks_num_cluster$Cluster_A_3 , main = "Clusters con kmeans modelo A_3")


hawks_num_cluster$DBSCAN_Cluster <- dbscan_extract_scaled$cluster
Hawks$DBSCAN_Cluster <- NA  # Crear una columna en Hawks para los clusters
Hawks[rownames(hawks_num_cluster), "DBSCAN_Cluster"] <- hawks_num_cluster$DBSCAN_Cluster

plot_clusters(Hawks, "DBSCAN_Cluster")


```





Como conclusion, DBSCAN es la eleccion adecuada ya que tiene menos ruido, y los valores outliers o atipicos no generan un problema por su baja densidad y ayuda a detectar mejor los clusters. kmeans es interesante siempre (ya que es un modelo muy utilizado y útil) y es mas interesante en datos que tienen una estructura mas simple, y si son datos circulares su distribución al centroide, mejor aún.
Por tanto, yo creo que DBSCAN captura mejor la estructura de datos, viendo el ejercicio se ve como he probado muchas combinaciones para kmeans, mas las que borré y reescribí, sin embargo con DBSCAN ha sido mucho más rápido encontrar una distribución de clusters adecuada y funciona mucho mejor. Además no funciona de forma aleatoria como en kmeans la selección de los centroides.Por tanto DBSCAN es claramente mucho mas robusto en este dataset.



Referencias:
Laboratorio en Python y R.
Mineria de datos.
datacamp.com
openclassrooms.com

